from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from app.database import get_db
from app import models, schemas
from app.api.auth import get_current_user
from pydantic import BaseModel  # <-- 1. Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Pydantic

# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­ (app.engines)
from app.engines.gmaps_collector import GmapsEngine
from app.engines.data_enricher import DataEnricher
from app.engines.verifier_pro import EmailVerifier

import time

router = APIRouter()

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø´Ø§Ù…Ù„Ø© (ØªÙ†ÙØ° ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©) ---
def run_full_scraping_task(keyword: str, location: str, user_id: int, db: Session, limit: int):
    print(f"ðŸš€ Ø¨Ø¯Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù€: {keyword} ÙÙŠ {location}")
    
    gmaps = GmapsEngine()
    enricher = DataEnricher()
    verifier = EmailVerifier()
    
    try:
        # 1. Ø³Ø­Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ø®Ø±Ø§Ø¦Ø· Ø¬ÙˆØ¬Ù„
        raw_results = gmaps.scrape(keyword, location, max_leads=limit)
        
        if not raw_results:
            print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø®Ø±Ø§Ø¦Ø· Ø¬ÙˆØ¬Ù„.")
            return

        # Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¹Ù† Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª ÙˆØµÙ†Ø§Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø±
        enricher.start_session()
        
        leads_saved = 0
        for item in raw_results:
            # 2. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¹Ù† Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„ ÙˆØµØ§Ù†Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ÙƒÙ„ Ø´Ø±ÙƒØ©
            extra_data = enricher.find_emails_and_people(item['company_name'], item['website'])
            
            # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„ ØªÙ‚Ù†ÙŠØ§Ù‹
            email_status, confidence = verifier.verify(extra_data['email'])
            
            # 4. Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            new_lead = models.Lead(
                user_id=user_id,
                company_name=item['company_name'],
                industry=keyword,
                location=item['location'],
                phone=item['phone'],
                website=item['website'],
                email=extra_data['email'],
                email_status=email_status,
                confidence_score=confidence,
                decision_maker_name=extra_data['decision_maker_name'],
                decision_maker_role=extra_data['decision_maker_role'],
                linkedin_url=extra_data['linkedin_url']
            )
            db.add(new_lead)
            db.commit()
            leads_saved += 1
            print(f"âœ… [SAVED] ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ­ÙØ¸: {item['company_name']}")

        enricher.stop_session()
        
        # 5. ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙÙŠ Ø³Ø¬Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø­Ø«
        history = models.SearchHistory(
            user_id=user_id,
            keyword=keyword,
            location=location,
            results_count=leads_saved
        )
        db.add(history)
        db.commit()

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙ†ÙŠ ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: {e}")
        db.rollback()
    finally:
        # Ø¶Ù…Ø§Ù† Ø¥ØºÙ„Ø§Ù‚ Ø¬Ù„Ø³Ø© Ø§Ù„Ù…ØªØµÙØ­ ÙÙŠ ÙƒÙ„ Ø§Ù„Ø£Ø­ÙˆØ§Ù„
        try:
            enricher.stop_session()
        except:
            pass

# --- 2. ØªØ¹Ø±ÙŠÙ Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© (Schema) ---
class SearchRequest(BaseModel):
    keyword: str
    location: str
    target_limit: int = 5

# --- Ù†Ù‚Ø·Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« ---
@router.post("/start-search/")
def start_search(
    request: SearchRequest,  # <-- 3. Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ€ JSON Body
    background_tasks: BackgroundTasks = BackgroundTasks(),
    db: Session = Depends(get_db),
    current_user: models.User = Depends(get_current_user)
):
    # 1. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø±ØµÙŠØ¯ ÙƒØ§ÙÙ (Ù†Ø³ØªØ®Ø¯Ù… request.target_limit)
    if current_user.credits < request.target_limit:
        raise HTTPException(status_code=400, detail="Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø±ØµÙŠØ¯Ùƒ ØºÙŠØ± ÙƒØ§ÙÙ Ù„Ø¥ØªÙ…Ø§Ù… Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.")

    # 2. ØªÙØ¹ÙŠÙ„ Ø®ØµÙ… Ø§Ù„Ø±ØµÙŠØ¯ ÙÙˆØ±Ø§Ù‹ Ù…Ù† Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    current_user.credits -= request.target_limit
    db.commit() # Ø­ÙØ¸ Ø§Ù„Ø®ØµÙ… ÙÙˆØ±Ø§Ù‹ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙ„Ø§Ø¹Ø¨
    print(f"ðŸ’° ØªÙ… Ø®ØµÙ… {request.target_limit} ØªÙˆÙƒÙ† Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {current_user.email}")

    # 3. Ø¥Ø·Ù„Ø§Ù‚ Ù…Ù‡Ù…Ø© Ø§Ù„Ø³Ø­Ø¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© (Ù†Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª request)
    background_tasks.add_task(
        run_full_scraping_task, 
        request.keyword, 
        request.location, 
        current_user.id, 
        db, 
        request.target_limit
    )
    
    return {
        "status": "success", 
        "message": f"Ø¨Ø¯Ø£ Ø§Ù„Ø¨Ø­Ø« Ø¨Ù†Ø¬Ø§Ø­ØŒ ØªÙ… Ø®ØµÙ… {request.target_limit} ØªÙˆÙƒÙ† Ù…Ù† Ø±ØµÙŠØ¯Ùƒ. Ø³ØªØ¸Ù‡Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„."
    }

# --- Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ© ---
@router.get("/my-leads/")
def get_my_leads(db: Session = Depends(get_db), current_user: models.User = Depends(get_current_user)):
    leads = db.query(models.Lead).filter(models.Lead.user_id == current_user.id).order_by(models.Lead.id.desc()).all()
    return {"data": leads}

# --- Ø¬Ù„Ø¨ Ø³Ø¬Ù„ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø« ---
@router.get("/history")
def get_history(db: Session = Depends(get_db), current_user: models.User = Depends(get_current_user)):
    return db.query(models.SearchHistory).filter(models.SearchHistory.user_id == current_user.id).all()